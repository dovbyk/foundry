#Communication with external LLM 

import os 
import json
from .schemas import get_schema_for_recipe
import google.generativeai as genai 
import asyncio 

genai.configure(api_key = os.environ["GEMINI_API_KEY"]) 

SYSTEM_PROMPT_TEMPLATE = """
You are an expert data curation assistant. Your task is to generate high-quality, structured data from the user-provided text chunk based on the specified recipe.

You must adhere to the following rules:
1.  Base your output *only* on the information present in the text chunk. Do not add any external knowledge.
2.  Your response MUST be a single, valid JSON object that strictly adheres to the provided JSON schema. Do not add any extra text or explanations.

RECIPE: {recipe_name}
JSON SCHEMA:
{json_schema}
"""

async def generate_data_from_chunk(chunk: str, recipe_name: str) -> dict | None:
    try:
        schema = get_schema_for_recipe(recipe_name)
        system_prompt = SYSTEM_PROMPT_TEMPLATE.format(
            recipe_name = recipe_name, 
            json_schema = json.dumps(schema, indent=2)
        )
        model = genai.GenerativeModel(
            model_name = "gemini-1.5-flash",
            system_instruction = system_prompt
        )

        generation_config = genai.types.GenerationConfig(
            response_mime_type = "application/json"
        )
        user_prompt = f"Here is the text chunk:\n\n---\n{chunk}\n---"

        response = await model.generate_content_async(
            user_prompt,
            generation_config = generation_config
        )

        json_output = response.text #text generated by LLM 

        return json.loads(json_output)#Expects LLM to generate JSON and parses it into python dict. If LLM didn't generate valid JSON, this will raise JSAONDecodeError
    
    except Exception as e:
        print(f"LLM generation error {e}")
        return None 





